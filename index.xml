<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NVIDIA Learning and Perception Research</title>
    <link>https://nvlabs.github.io/</link>
      <atom:link href="https://nvlabs.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>NVIDIA Learning and Perception Research</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://nvlabs.github.io/images/logo_hu2fe6632db44d28c9b9d53edd3914c1d6_112452_300x300_fit_lanczos_3.png</url>
      <title>NVIDIA Learning and Perception Research</title>
      <link>https://nvlabs.github.io/</link>
    </image>
    
    <item>
      <title>GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras</title>
      <link>https://nvlabs.github.io/publication/yuan2022glamr/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/yuan2022glamr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects</title>
      <link>https://nvlabs.github.io/publication/noguchi2022watch/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/noguchi2022watch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Binary TTC: A Temporal Geofence for Autonomous Navigation</title>
      <link>https://nvlabs.github.io/publication/badki2021bittc/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/badki2021bittc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Two-View Structure-from-Motion Revisited</title>
      <link>https://nvlabs.github.io/publication/zhong2021sfm/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/zhong2021sfm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DexYCB: A Benchmark for Capturing Hand Grasping of Objects</title>
      <link>https://nvlabs.github.io/publication/chao2021dexycb/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/chao2021dexycb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fast Uncertainty Quantification for Deep Object Pose Estimation</title>
      <link>https://nvlabs.github.io/publication/shi2021fastuq/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/shi2021fastuq/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hierarchical Planning for Long-Horizon Manipulation with Geometric and Symbolic Scene Graphs</title>
      <link>https://nvlabs.github.io/publication/zhu2021hier/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/zhu2021hier/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joint Space Control via Deep Reinforcement Learning</title>
      <link>https://nvlabs.github.io/publication/kumar2021jailer/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/kumar2021jailer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KAMA: 3D Keypoint Aware Body Mesh Articulation</title>
      <link>https://nvlabs.github.io/publication/iqbal2021kama/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/iqbal2021kama/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-View Fusion for Multi-Level Robotic Scene Understanding</title>
      <link>https://nvlabs.github.io/publication/lin2021mvml/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/lin2021mvml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NViSII: A Scriptable Tool for Photorealistic Image Generation</title>
      <link>https://nvlabs.github.io/publication/morrical2021nvisii/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/morrical2021nvisii/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RMPflow: A Geometric Framework for Generation of Multi-Task Motion Policies</title>
      <link>https://nvlabs.github.io/publication/cheng2021rmpflow/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/cheng2021rmpflow/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-Supervised Real-to-Sim Scene Generation</title>
      <link>https://nvlabs.github.io/publication/prakash2021iccvrealtosim/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/prakash2021iccvrealtosim/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bi3D: Stereo Depth Estimation via Binary Classifications</title>
      <link>https://nvlabs.github.io/publication/badki2020bi3d/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/badki2020bi3d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NVAE: A Deep Hierarchical Variational Autoencoder</title>
      <link>https://nvlabs.github.io/publication/vahdat2020nvae/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/vahdat2020nvae/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild</title>
      <link>https://nvlabs.github.io/publication/iqbal2020weakpose/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/publication/iqbal2020weakpose/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LPR Group</title>
      <link>https://nvlabs.github.io/author/lpr-group/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nvlabs.github.io/author/lpr-group/</guid>
      <description>&lt;h1 id=&#34;nvidia-learning-and-perception-research-group&#34;&gt;NVIDIA Learning and Perception Research Group&lt;/h1&gt;
&lt;p&gt;Welcome to the homepage of NVIDIA&amp;rsquo;s Learning and Perception Research group, led by &lt;a href=&#34;https://jankautz.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr. Jan Kautz&lt;/a&gt;. Our research spans computer vision and machine learning. We are particularly interested in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Human-centric Perception&lt;/em&gt;: body pose, hand pose, facial landmarks, gaze estimation, activity detection and recognition, etc.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Perception for Autonomous Machines&lt;/em&gt;: stereo, optical flow, object pose estimation, 3D shape estimation, etc.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Neural Content Capture and Synthesis&lt;/em&gt;: image and view synthesis, neural avatars, neural agents, denoising diffusion models, GANs, etc.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Resource-Efficient Deep Learning&lt;/em&gt;: pruning, NAS, efficient backbones, weakly- and self-supervised learning, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- (We are always looking for outstanding research scientist, please reach out to us if you are interested in joining our team.) --&gt;
&lt;p&gt;Graduate students interested in interning with us are welcome reach out directly to team members for more details.&lt;/p&gt;
&lt;!-- Please see [this link](https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite) for open positions. --&gt;
&lt;!-- You can also contact our [members](#people) directly for potential internship positions. --&gt;
</description>
    </item>
    
  </channel>
</rss>
